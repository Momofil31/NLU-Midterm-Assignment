{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb67ba8d",
   "metadata": {},
   "source": [
    "# NLU: Mid-Term Assignment 2022\n",
    "\n",
    "### Description\n",
    "\n",
    "In this notebook, we ask you to complete four main tasks to show what you have learnt during the NLU labs. Therefore, to complete the assignment please refer to the concepts, libraries and other materials shown and used during the labs. The last task is not mandatory, it is a _BONUS_ to get an extra mark for the laude.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "-   **Dataset**: in this notebook, you are asked to work with the dataset _Conll 2003_ provided by us in the _data_ folder. Please, load the files from the _data_ folder and **do not** change names or paths of the inner files.\n",
    "-   **Output**: for each part of your task, print your results and leave it in the notebook. Please, **do not** send a jupyter notebook without the printed outputs.\n",
    "-   **Other**: follow carefully all the further instructions and suggestions given in the question descriptions.\n",
    "\n",
    "### Deadline\n",
    "\n",
    "The deadline is due in two weeks from the project presentation. Please, refer to _piazza_ channel for the exact date.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996d123d",
   "metadata": {},
   "source": [
    "### Task 1: Analysis of the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ead0d1f",
   "metadata": {},
   "source": [
    "#### Q 1.1\n",
    "\n",
    "-   Create the Vocabulary and Frequency Dictionary of the:\n",
    "    1. Whole dataset\n",
    "    2. Train set\n",
    "    3. Test set\n",
    "\n",
    "**Attention**: print the first 20 words of the Dictionaty of each set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca1124f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train vocab length: 21009\n",
      "Test vocab length: 8548\n",
      "Whole vocab length: 26869\n",
      "Train 20 most common: [('the', 8390), ('.', 7374), (',', 7290), ('of', 3815), ('in', 3621), ('to', 3424), ('a', 3199), ('and', 2872), ('(', 2861), (')', 2861), ('\"', 2178), ('on', 2092), ('said', 1849), (\"'s\", 1566), ('for', 1465), ('1', 1421), ('-', 1243), ('at', 1146), ('was', 1095), ('2', 973)]\n",
      "Test 20 most common: [('the', 1765), (',', 1637), ('.', 1626), ('to', 805), ('of', 789), ('in', 761), ('(', 686), (')', 684), ('a', 658), ('and', 598), ('on', 467), ('\"', 421), ('said', 399), (\"'s\", 347), ('-', 287), ('for', 286), ('at', 251), ('was', 224), ('4', 201), ('with', 185)]\n",
      "Whole 20 most common: [('the', 12310), (',', 10876), ('.', 10874), ('of', 5502), ('in', 5405), ('to', 5129), ('a', 4731), ('(', 4226), (')', 4225), ('and', 4223), ('\"', 3239), ('on', 3115), ('said', 2694), (\"'s\", 2339), ('for', 2109), ('-', 1866), ('1', 1845), ('at', 1679), ('was', 1593), ('2', 1342)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus.reader import ConllCorpusReader\n",
    "from nltk import FreqDist\n",
    "\n",
    "TRAIN = \"train.txt\"\n",
    "VALID = \"valid.txt\"\n",
    "TEST = \"test.txt\"\n",
    "WHOLE = [TRAIN, VALID, TEST]\n",
    "\n",
    "# Read dataset\n",
    "conll2003 = ConllCorpusReader(\"data/\", \".txt\", ('words', 'pos', 'ignore', 'chunk'))\n",
    "train_words = [word.lower() for word in conll2003.words(TRAIN)]\n",
    "test_words = [word.lower() for word in conll2003.words(TEST)]\n",
    "whole_words = [word.lower() for word in conll2003.words(WHOLE)]\n",
    "\n",
    "# Create vocabularies\n",
    "train_vocab = set(train_words)\n",
    "test_vocab = set(test_words)\n",
    "whole_vocab = set(whole_words)\n",
    "\n",
    "print(f\"Train vocab length: {len(train_vocab)}\")\n",
    "print(f\"Test vocab length: {len(test_vocab)}\")\n",
    "print(f\"Whole vocab length: {len(whole_vocab)}\")\n",
    "\n",
    "# Create Frequency Dictionaries\n",
    "train_freqdist = FreqDist(train_words)\n",
    "test_freqdist = FreqDist(test_words)\n",
    "whole_freqdist = FreqDist(whole_words)\n",
    "\n",
    "print(f\"Train 20 most common: {train_freqdist.most_common(20)}\")\n",
    "print(f\"Test 20 most common: {test_freqdist.most_common(20)}\")\n",
    "print(f\"Whole 20 most common: {whole_freqdist.most_common(20)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0dc02f",
   "metadata": {},
   "source": [
    "#### Q 1.2\n",
    "\n",
    "-   Obtain the list of:\n",
    "    1. Out-Of-Vocabulary (OOV) tokens\n",
    "    2. Overlapping tokens between train and test sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b660bcb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of OOV tokens: 3268\n",
      "Number of overlapping tokens: 5280\n"
     ]
    }
   ],
   "source": [
    "# Compute OOV tokens are tokens which are present in the test set but not in the training set\n",
    "OOV_tokens = list(test_vocab.difference(train_vocab))\n",
    "\n",
    "# Compute overlapping tokens\n",
    "overlapping_tokens = list(train_vocab.intersection(test_vocab))\n",
    "print(f\"Number of OOV tokens: {len(OOV_tokens)}\")\n",
    "print(f\"Number of overlapping tokens: {len(overlapping_tokens)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab1ac1c",
   "metadata": {},
   "source": [
    "#### Q 1.3\n",
    "\n",
    "-   Perform a complete data analysis of the whole dataset (train + test sets) to obtain:\n",
    "    1. Average sentence length computed in number of tokens\n",
    "    2. The 50 most-common tokens\n",
    "    3. Number of sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "36e5c39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sentence length: 13.62\n",
      "50 most common tokens: ['the', ',', '.', 'of', 'in', 'to', 'a', '(', ')', 'and', '\"', 'on', 'said', \"'s\", 'for', '-', '1', 'at', 'was', '2', 'with', '3', '0', 'that', 'he', 'from', 'by', 'it', ':', 'is', '4', 'as', 'his', 'had', 'were', 'an', 'but', 'not', 'after', 'has', 'be', 'have', 'new', 'first', 'who', '5', 'will', '6', 'two', 'they']\n",
      "Number of sentences: 22137\n"
     ]
    }
   ],
   "source": [
    "def avg_sentence_length(sents):\n",
    "    '''\n",
    "    Compute average length of a sentence where the length of a sentence is the number of token it is made of.\n",
    "    params:\n",
    "        sents: List of sentences \n",
    "\n",
    "    '''\n",
    "    sent_lens = [len(sent) for sent in whole_sents]\n",
    "    return sum(sent_lens)/len(sent_lens)\n",
    "\n",
    "# Get list of sentences\n",
    "whole_sents = conll2003.sents(WHOLE)\n",
    "\n",
    "# Compute average sentence length\n",
    "avg_sent_len_whole = avg_sentence_length(whole_sents)\n",
    "print(f\"Average sentence length: {round(avg_sent_len_whole, 2)}\")\n",
    "\n",
    "# Compute 50 most common token in the whole dataset\n",
    "most_common_50 = [token for token, _ in whole_freqdist.most_common(50)]\n",
    "print(f\"50 most common tokens: {most_common_50}\")\n",
    "\n",
    "# Compute the number of sentences in the whole dataset\n",
    "num_sentences = len(whole_sents)\n",
    "print(f\"Number of sentences: {num_sentences}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726af097",
   "metadata": {},
   "source": [
    "#### Q 1.4\n",
    "\n",
    "-   Create the dictionary of Named Entities and their Frequencies for the:\n",
    "    1. Whole dataset\n",
    "    2. Train set\n",
    "    3. Test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5659670d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('U.S.', 303), ('Germany', 141), ('Britain', 133), ('Australia', 130), ('England', 123), ('France', 122), ('Spain', 110), ('Italy', 98), ('NEW YORK', 95), ('LONDON', 93)]\n",
      "[('Germany', 49), ('U.S.', 45), ('Australia', 45), ('Japan', 41), ('Italy', 41), ('France', 40), ('World Cup', 34), ('Russia', 34), ('Indonesia', 33), ('China', 32)]\n",
      "[('U.S.', 460), ('Germany', 237), ('Australia', 204), ('France', 199), ('England', 176), ('Russia', 167), ('Britain', 165), ('Italy', 160), ('China', 149), ('LONDON', 147)]\n"
     ]
    }
   ],
   "source": [
    "# Each list contains tuples (word, POS_tag, IOB)\n",
    "train_iob_words = conll2003.iob_words(TRAIN)\n",
    "test_iob_words = conll2003.iob_words(TEST)\n",
    "whole_iob_words = conll2003.iob_words(WHOLE)\n",
    "\n",
    "\n",
    "def get_named_entities(iob_words):\n",
    "    '''\n",
    "    Return list of named entities.\n",
    "    params:\n",
    "        iob_words: List of tuples (word, POS_tag, IOB)\n",
    "    return:\n",
    "        ne: List of named entities as strings (eg. 'Los Angeles')\n",
    "    '''\n",
    "    ne = []\n",
    "    entity = \"\"\n",
    "    prev_label = \"\"\n",
    "    for word, pos, iob in iob_words:\n",
    "        iob_split = iob.split('-')\n",
    "        iob = iob_split[0]\n",
    "        label = iob_split[1] if len(iob_split) > 1 else None\n",
    "        if label is not None:\n",
    "            if iob == \"B\":  # begin entity\n",
    "                if len(entity):\n",
    "                    ne.append(entity)\n",
    "                entity = word\n",
    "            elif iob == \"I\" and prev_label == label:\n",
    "                entity += \" \" + word\n",
    "        else:\n",
    "            if len(entity):\n",
    "                ne.append(entity)\n",
    "            entity = \"\"\n",
    "        prev_label = label\n",
    "\n",
    "    return ne\n",
    "\n",
    "\n",
    "train_ne_freqdist = FreqDist(get_named_entities(train_iob_words))\n",
    "test_ne_freqdist = FreqDist(get_named_entities(test_iob_words))\n",
    "whole_ne_freqdist = FreqDist(get_named_entities(whole_iob_words))\n",
    "\n",
    "print(train_ne_freqdist.most_common(10))\n",
    "print(test_ne_freqdist.most_common(10))\n",
    "print(whole_ne_freqdist.most_common(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a08f37",
   "metadata": {},
   "source": [
    "### Task 2: Working with Dependecy Tree\n",
    "\n",
    "_Suggestions: use Spacy pipeline to retreive the Dependecy Tree_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1ba597",
   "metadata": {},
   "source": [
    "#### Q 2.1\n",
    "\n",
    "-   Given each sentence in the dataset, write the required functions to provide:\n",
    "    1. Subject, obects (direct and indirect)\n",
    "    2. Noun chunks\n",
    "    3. The head noun in each noun chunk\n",
    "\n",
    "**Attention**: _print only the results of these functions by using the sentence \"I saw the man with a telescope\"_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f6292d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subjects: [('I', 'nsubj')]\n",
      "Objects: [('the man', 'dobj')]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Noun chunks</th>\n",
       "      <th>Head</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence</th>\n",
       "      <th>chunk</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>I</td>\n",
       "      <td>saw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the man</td>\n",
       "      <td>saw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a telescope</td>\n",
       "      <td>with</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Noun chunks  Head\n",
       "sentence chunk                   \n",
       "0        0                I   saw\n",
       "         1          the man   saw\n",
       "         2      a telescope  with"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_subjects(sent):\n",
    "    '''Given a Spacy sentencence return a list of subjects ('nsubj' tag) present in the sentence.'''\n",
    "    subjects = [(chunk.text, chunk.root.dep_) for chunk in sent.noun_chunks if chunk.root.dep_ == 'nsubj']\n",
    "    return subjects\n",
    "\n",
    "\n",
    "def get_objects(sent):\n",
    "    '''Given a Spacy sentencence return a list of direct and indirect objects ('dobj' and 'iobj' tags) present in the sentence.'''\n",
    "    objects = [(chunk.text, chunk.root.dep_) for chunk in sent.noun_chunks if chunk.root.dep_ == 'dobj' or chunk.root.dep_ == 'iobj']\n",
    "    return objects\n",
    "\n",
    "\n",
    "def get_noun_chunks(sent):\n",
    "    '''Return noun chunks list'''\n",
    "    noun_chunks = []\n",
    "    for i, chunk in enumerate(sent.noun_chunks):\n",
    "        noun_chunks.append(chunk.text)\n",
    "    return noun_chunks\n",
    "\n",
    "\n",
    "def get_noun_chunk_heads(sent):\n",
    "    '''Return noun chunk heads list'''\n",
    "    noun_chunk_heads = []\n",
    "    for i, chunk in enumerate(sent.noun_chunks):\n",
    "        noun_chunk_heads.append(chunk.root.head)\n",
    "    return noun_chunk_heads\n",
    "\n",
    "\n",
    "def get_indexes(sents):\n",
    "    '''Returns tuple of indexes to use for pandas dataframe'''\n",
    "    tuples = []\n",
    "    for sent_idx, sent in enumerate(sents):\n",
    "        for i, chunk in enumerate(sent.noun_chunks):\n",
    "            tuples.append((sent_idx, i))\n",
    "    return tuples\n",
    "\n",
    "\n",
    "example_sent = \"I saw the man with a telescope.\"\n",
    "spacy_nlp = en_core_web_sm.load()\n",
    "spacy_doc = spacy_nlp(example_sent)\n",
    "\n",
    "noun_chunks = []\n",
    "noun_chunk_heads = []\n",
    "\n",
    "for sent_idx, sent in enumerate(spacy_doc.sents):\n",
    "    # print objects and subjects of example sentence\n",
    "    print(f\"Subjects: {get_subjects(sent)}\")\n",
    "    print(f\"Objects: {get_objects(sent)}\")\n",
    "    noun_chunks.extend(get_noun_chunks(sent))\n",
    "    noun_chunk_heads.extend(get_noun_chunk_heads(sent))\n",
    "\n",
    "indexes = get_indexes(spacy_doc.sents)\n",
    "\n",
    "nc = dict()\n",
    "nc[\"Noun chunks\"] = noun_chunks\n",
    "nc[\"Head\"] = noun_chunk_heads\n",
    "\n",
    "indexes = pd.MultiIndex.from_tuples(indexes, names=[\"sentence\", \"chunk\"])\n",
    "df = pd.DataFrame(nc, index=indexes)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84838829",
   "metadata": {},
   "source": [
    "#### Q 2.2\n",
    "\n",
    "-   Given a dependecy tree of a sentence and a segment of that sentence write the required functions that ouput the dependency subtree of that segment.\n",
    "\n",
    "**Attention**: _print only the results of these functions by using the sentence \"I saw the man with a telescope\" (the segment could be any e.g. \"saw the man\", \"a telescope\", etc.)_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2767dfa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segment: 'saw the man'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"dab0ba163b49484785a8fba666fb2979-0\" class=\"displacy\" width=\"575\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">saw</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">man</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-dab0ba163b49484785a8fba666fb2979-0-0\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-dab0ba163b49484785a8fba666fb2979-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,179.0 L237,167.0 253,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-dab0ba163b49484785a8fba666fb2979-0-1\" stroke-width=\"2px\" d=\"M70,177.0 C70,2.0 400.0,2.0 400.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-dab0ba163b49484785a8fba666fb2979-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M400.0,179.0 L408.0,167.0 392.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "from spacy.tokens import Doc\n",
    "import en_core_web_sm\n",
    "\n",
    "\n",
    "def print_tree(doc: Doc, segment: str):\n",
    "    '''\n",
    "    Given a dependecy tree of a sentence and a segment of that sentence ouput the dependency subtree of that segment.\n",
    "    params:\n",
    "        doc: spacy Doc which contains the dependency tree\n",
    "        segment: segment (string) of which the dependency subtree should be displayed\n",
    "                 as stated multiple times by the T.A.s it is expected that the segment is a valid and existing chunk\n",
    "    '''\n",
    "\n",
    "    def find_sublist(list, sublist):\n",
    "        m = len(list)\n",
    "        n = len(sublist)\n",
    "        for i in range(m-n+1):\n",
    "            j = 0\n",
    "            while j < n:\n",
    "                if (list[i+j] != sublist[j]):\n",
    "                    break\n",
    "                j += 1\n",
    "            if j == n:\n",
    "                return i\n",
    "        return -1\n",
    "\n",
    "    segment_doc = spacy_nlp(segment)  # tokenize segment\n",
    "    doc_tokens = [token.text for token in doc]  # list of tokens in the doc\n",
    "    segment_tokens = [token.text for token in segment_doc]  # list of tokens in the segment\n",
    "\n",
    "    # check if segment exist in doc and return its position\n",
    "    start = find_sublist(doc_tokens, segment_tokens)\n",
    "    if start == -1:\n",
    "        print(\"Segment is invalid\")\n",
    "        return\n",
    "\n",
    "    # extract chunk from the doc and display the subtree\n",
    "    chunk = doc[start:start+len(segment_doc)]\n",
    "    displacy.render(chunk, style=\"dep\")\n",
    "\n",
    "\n",
    "example_sent = \"I saw the man with a telescope\"\n",
    "example_segment = 'saw the man'\n",
    "\n",
    "spacy_nlp = en_core_web_sm.load()\n",
    "doc = spacy_nlp(example_sent)\n",
    "\n",
    "print(f\"Segment: '{example_segment}'\")\n",
    "print_tree(doc, example_segment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292e99ac",
   "metadata": {},
   "source": [
    "#### Q 2.3\n",
    "\n",
    "-   Given a token in a sentence, write the required functions that output the dependency path from the root of the dependency tree to that given token.\n",
    "\n",
    "**Attention**: _print only the results of these functions by using the sentence \"I saw the man with a telescope\"_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3b0b1106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 'I', dependency path: saw -> I\n",
      "Token: 'saw', dependency path: saw\n",
      "Token: 'the', dependency path: saw -> man -> the\n",
      "Token: 'man', dependency path: saw -> man\n",
      "Token: 'with', dependency path: saw -> man -> with\n",
      "Token: 'a', dependency path: saw -> man -> with -> telescope -> a\n",
      "Token: 'telescope', dependency path: saw -> man -> with -> telescope\n"
     ]
    }
   ],
   "source": [
    "def print_dependency_path(doc: Doc, token_str: str):\n",
    "    '''\n",
    "    Given a token in a sentence (as string), output the dependency path from the root of the dependency tree to that given token.\n",
    "    params:\n",
    "        doc: spacy Doc with the dependency tree\n",
    "        token_str: token to print the dependency path of\n",
    "    '''\n",
    "    # check if token exist in document\n",
    "    for token in doc:\n",
    "        if token.text == token_str:\n",
    "            # compute dependency path from root to token\n",
    "            path = reversed([token, *token.ancestors])\n",
    "            print(*(path), sep=\" -> \")\n",
    "            return\n",
    "    print(\"Token not found\")\n",
    "\n",
    "\n",
    "example_sent = \"I saw the man with a telescope\"\n",
    "spacy_nlp = en_core_web_sm.load()\n",
    "spacy_doc = spacy_nlp(example_sent)\n",
    "\n",
    "for sent in spacy_doc.sents:\n",
    "    for token in sent:\n",
    "        print(f\"Token: '{token.text}', dependency path:\", end=\" \")\n",
    "        print_dependency_path(spacy_doc, token.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3358779",
   "metadata": {},
   "source": [
    "### Task 3: Named Entity Recognition\n",
    "\n",
    "_Suggestion: use scikit-learn metric functions. See classification_report_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c820ad69",
   "metadata": {},
   "source": [
    "#### Q 3.1\n",
    "\n",
    "-   Benchmark Spacy Named Entity Recognition model on the test set by:\n",
    "    1. Providing the list of categories in the dataset (person, organization, etc.)\n",
    "    2. Computing the overall accuracy on NER\n",
    "    3. Computing the performance of the Named Entity Recognition model for each category:\n",
    "        - Compute the perfomance at the token level (eg. B-Person, I-Person, B-Organization, I-Organization, O, etc.)\n",
    "        - Compute the performance at the entity level (eg. Person, Organization, etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "90ed2d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset NE categories: {'ORG', 'MISC', 'PER', 'LOC'}\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "\n",
    "def get_ne_label_from_iob(words_iob):\n",
    "    '''\n",
    "    Returns list of named entity category label for each word in the list of tuples (word, pos, iob) passed, excluding O.\n",
    "    '''\n",
    "    ne = []\n",
    "    for _, _, iob in words_iob:\n",
    "        iob_split = iob.split('-')\n",
    "        if len(iob_split) > 1:\n",
    "            ne.append(iob_split[1])\n",
    "    return ne\n",
    "\n",
    "\n",
    "# 1. Provide the list of categories in the dataset\n",
    "words_iob = conll2003.iob_words(TEST)\n",
    "categories = set(get_ne_label_from_iob(words_iob))\n",
    "print(f\"Dataset NE categories: {categories}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "469ce489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get spacy NE hypothesis (mapped to conll2003 categories)\n",
    "\n",
    "# map spacy NE categories to Conll2003 NE categories\n",
    "# The choice of the mapping is based on the output of spacy.explain(\"MISC\") and spacy.explain(<en_cor_web_sm category label>)\n",
    "# MISC: 'Miscellaneous entities, e.g. events, nationalities, products or works of art'\n",
    "# All the other categories are dropped\n",
    "spacy2conll = {\n",
    "    \"FAC\": \"MISC\",\n",
    "    \"GPE\": \"LOC\",\n",
    "    \"LANGUAGE\": \"MISC\",\n",
    "    \"LOC\": \"LOC\",\n",
    "    \"NORP\": \"MISC\",\n",
    "    \"ORG\": \"ORG\",\n",
    "    \"PERSON\": \"PER\",\n",
    "    \"PRODUCT\": \"MISC\",\n",
    "    \"WORK_OF_ART\": \"MISC\",\n",
    "}\n",
    "\n",
    "\n",
    "def join_label(iob, label, oos=None):\n",
    "    '''Join IOB with NE category as conll2003 format and maps NE categories into conll2003 categories.'''\n",
    "    oos = 'O' if oos is None else oos\n",
    "    if iob == oos:\n",
    "        return oos\n",
    "    elif label not in spacy2conll:\n",
    "        return oos\n",
    "    else:\n",
    "        return \"-\".join([iob, spacy2conll.get(label)])\n",
    "\n",
    "\n",
    "def get_sent_starts(conll_sents):\n",
    "    '''Returns list of booleans which represent if the corresponding token is the start of a sentence.'''\n",
    "    sent_starts = []\n",
    "    for sent in conll_sents:\n",
    "        is_start = True\n",
    "        for token in sent:\n",
    "            sent_starts.append(is_start)\n",
    "            is_start = False\n",
    "    return sent_starts\n",
    "\n",
    "\n",
    "def get_ne_hyps(doc: Doc):\n",
    "    '''Returns list of NE (one for each token) hypothesis recognized by Spacy, already mapped into Conll2003 subset of categories.'''\n",
    "    ne_hyp = []\n",
    "    for sent in doc.sents:\n",
    "        sent_labels = []\n",
    "        for token in sent:\n",
    "            label = join_label(token.ent_iob_, token.ent_type_)\n",
    "            sent_labels.append((token.text, label))\n",
    "        ne_hyp.append(sent_labels)\n",
    "    return ne_hyp\n",
    "\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "ner_test_words = [word for word in conll2003.words(TEST)]\n",
    "sents = [sent for sent in conll2003.sents(TEST) if len(sent)] # get list of sentences tokenized\n",
    "doc = Doc(nlp.vocab, words=ner_test_words, sent_starts=get_sent_starts(sents)) # create Doc from Conll2003 tokens.\n",
    "doc = nlp(doc) # Process doc with Spacy\n",
    "\n",
    "ne_hyps = get_ne_hyps(doc) # Get Spacy hypothesis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "14de9461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 + 3.a Performace of Spacy NE recognition at token level, for each category and overall:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.79      0.69      0.74      1668\n",
      "      B-MISC       0.72      0.51      0.60       702\n",
      "       B-ORG       0.53      0.26      0.35      1661\n",
      "       B-PER       0.81      0.55      0.66      1617\n",
      "       I-LOC       0.62      0.57      0.60       257\n",
      "      I-MISC       0.23      0.09      0.13       216\n",
      "       I-ORG       0.50      0.46      0.48       835\n",
      "       I-PER       0.83      0.66      0.74      1156\n",
      "           O       0.93      0.99      0.96     38323\n",
      "\n",
      "    accuracy                           0.90     46435\n",
      "   macro avg       0.66      0.53      0.58     46435\n",
      "weighted avg       0.89      0.90      0.89     46435\n",
      "\n",
      "2 + 3.b Performace of Spacy NE recognition at entity level, for each category and overall:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "      <th>f</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ORG</th>\n",
       "      <td>0.469</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.309</td>\n",
       "      <td>1661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MISC</th>\n",
       "      <td>0.712</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.587</td>\n",
       "      <td>702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PER</th>\n",
       "      <td>0.774</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.629</td>\n",
       "      <td>1617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOC</th>\n",
       "      <td>0.784</td>\n",
       "      <td>0.682</td>\n",
       "      <td>0.729</td>\n",
       "      <td>1668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>0.706</td>\n",
       "      <td>0.483</td>\n",
       "      <td>0.573</td>\n",
       "      <td>5648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           p      r      f     s\n",
       "ORG    0.469  0.230  0.309  1661\n",
       "MISC   0.712  0.500  0.587   702\n",
       "PER    0.774  0.530  0.629  1617\n",
       "LOC    0.784  0.682  0.729  1668\n",
       "total  0.706  0.483  0.573  5648"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute and display performances\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "# conll.py should be in a src folder\n",
    "sys.path.insert(0, os.path.abspath('./src/'))\n",
    "from conll import evaluate\n",
    "\n",
    "# get ground truth IOB NE from the dataset\n",
    "refs = [[(text, iob) for text, pos, iob in sent] for sent in conll2003.iob_sents(TEST) if len(sent)]\n",
    "\n",
    "# 2 + 3.a compute performace of Spacy NE recognition at token level for each category and overall:\n",
    "refs_token_level = [iob for sent in refs for _, iob in sent]\n",
    "hyps_token_level = [iob for sent in ne_hyps for _, iob in sent]\n",
    "\n",
    "token_level_report = classification_report(refs_token_level, hyps_token_level)\n",
    "print(\"2 + 3.a Performace of Spacy NE recognition at token level, for each category and overall:\")\n",
    "print(token_level_report)\n",
    "\n",
    "# 2 + 3.b Compute performance of Spacy NE recognition at entity level for each category and overall\n",
    "# using evaluate() function defined in conll.py as we saw in Lab07\n",
    "print(\"2 + 3.b Performace of Spacy NE recognition at entity level, for each category and overall:\")\n",
    "entity_level_report = evaluate(refs, ne_hyps)\n",
    "\n",
    "pd_entity_level_report = pd.DataFrame().from_dict(entity_level_report, orient='index')\n",
    "pd_entity_level_report.round(decimals=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d669ee84",
   "metadata": {},
   "source": [
    "### Task 4: BONUS PART (extra mark for laude)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f56fc4f",
   "metadata": {},
   "source": [
    "#### Q 4.1\n",
    "\n",
    "-   Modify NLTK Transition parser's Configuration calss to use better features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8b182ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse.transitionparser import Configuration\n",
    "from nltk.parse.transitionparser import TransitionParser\n",
    "from nltk.parse.transitionparser import Transition\n",
    "import pickle\n",
    "import tempfile\n",
    "from copy import deepcopy\n",
    "from operator import itemgetter\n",
    "from os import remove\n",
    "\n",
    "try:\n",
    "    from numpy import array\n",
    "    from scipy import sparse\n",
    "    from sklearn.datasets import load_svmlight_file\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "from nltk.parse import DependencyEvaluator, DependencyGraph, ParserI\n",
    "\n",
    "\n",
    "class BetterConfiguration(Configuration):\n",
    "    '''\n",
    "    Extend nltk.parse.transitionparser.Configuration to override extract_features function\n",
    "    in order to extract better features.\n",
    "    '''\n",
    "\n",
    "    def extract_features(self):\n",
    "        '''\n",
    "        Based on the nltk implementation of this function, extract features based on the feature template proposed in:\n",
    "        Yue Zhang and Joakim Nivre. 2011. Transition-based Dependency Parsing with Rich Non-local Features. \n",
    "        In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, \n",
    "        pages 188–193, Portland, Oregon, USA. Association for Computational Linguistics.\n",
    "\n",
    "        Only Table 1 baseline feature template are used. Original nltk features are not removed.\n",
    "        '''\n",
    "        result = []\n",
    "        S_0w = S_0p = S_0wp = None\n",
    "        N_0wp = N_0w = N_0p = None\n",
    "        N_1p = None\n",
    "        N_2p = None\n",
    "        S_0h_p = S_0l_p = S_0r_p = None\n",
    "\n",
    "        if len(self.stack) > 0:\n",
    "            # Stack 0\n",
    "            stack_idx0 = self.stack[len(self.stack) - 1]\n",
    "            token = self._tokens[stack_idx0]\n",
    "\n",
    "            # S_0w\n",
    "            if self._check_informative(token[\"word\"], True):\n",
    "                S_0w = \"STK_0_FORM_\" + token[\"word\"]\n",
    "                result.append(S_0w)\n",
    "            if \"lemma\" in token and self._check_informative(token[\"lemma\"]):\n",
    "                result.append(\"STK_0_LEMMA_\" + token[\"lemma\"])\n",
    "            # S_0p\n",
    "            if self._check_informative(token[\"tag\"]):\n",
    "                S_0p = \"STK_0_POS_\" + token[\"tag\"]\n",
    "                result.append(S_0p)\n",
    "            if \"feats\" in token and self._check_informative(token[\"feats\"]):\n",
    "                feats = token[\"feats\"].split(\"|\")\n",
    "                for feat in feats:\n",
    "                    result.append(\"STK_0_FEATS_\" + feat)\n",
    "            # ADDED S_0wp\n",
    "            if self._check_informative(token[\"word\"], True) and self._check_informative(token[\"tag\"]):\n",
    "                S_0wp = \"STK_0_FORM_POS_\" + token[\"word\"] + \"_\" + token[\"tag\"]\n",
    "                result.append(S_0wp)\n",
    "            \n",
    "            # Extract head of the token\n",
    "            if token[\"head\"] is not None:\n",
    "                head = self._tokens[token[\"head\"]]\n",
    "                if self._check_informative(head[\"tag\"]):\n",
    "                    S_0h_p = \"STK_0_HEAD_POS_\" + head[\"tag\"]\n",
    "            \n",
    "            # Get leftmost & rightmost child of the token:\n",
    "            if len(token[\"deps\"][\"\"]):\n",
    "                leftmost = self._tokens[token[\"deps\"][\"\"][0]]\n",
    "                rightmost = self._tokens[token[\"deps\"][\"\"][-1]] \n",
    "                if self._check_informative(leftmost[\"tag\"]):\n",
    "                    S_0l_p = \"STK_0_LCHILD_POS_\" + leftmost[\"tag\"]\n",
    "                if self._check_informative(rightmost[\"tag\"]):\n",
    "                    S_0r_p = \"STK_0_RCHILD_POS_\" + rightmost[\"tag\"]\n",
    "\n",
    "            # Stack 1\n",
    "            if len(self.stack) > 1:\n",
    "                stack_idx1 = self.stack[len(self.stack) - 2]\n",
    "                token = self._tokens[stack_idx1]\n",
    "                if self._check_informative(token[\"tag\"]):\n",
    "                    result.append(\"STK_1_POS_\" + token[\"tag\"])\n",
    "\n",
    "            # Left most, right most dependency of stack[0]\n",
    "            left_most = 1000000\n",
    "            right_most = -1\n",
    "            dep_left_most = \"\"\n",
    "            dep_right_most = \"\"\n",
    "            for (wi, r, wj) in self.arcs:\n",
    "                if wi == stack_idx0:\n",
    "                    if (wj > wi) and (wj > right_most):\n",
    "                        right_most = wj\n",
    "                        dep_right_most = r\n",
    "                    if (wj < wi) and (wj < left_most):\n",
    "                        left_most = wj\n",
    "                        dep_left_most = r\n",
    "            if self._check_informative(dep_left_most):\n",
    "                result.append(\"STK_0_LDEP_\" + dep_left_most)\n",
    "            if self._check_informative(dep_right_most):\n",
    "                result.append(\"STK_0_RDEP_\" + dep_right_most)\n",
    "\n",
    "        # Check Buffered 0\n",
    "        if len(self.buffer) > 0:\n",
    "            # Buffer 0\n",
    "            buffer_idx0 = self.buffer[0]\n",
    "            token = self._tokens[buffer_idx0]\n",
    "            # N_0w\n",
    "            if self._check_informative(token[\"word\"], True):\n",
    "                N_0w = \"BUF_0_FORM_\" + token[\"word\"]\n",
    "                result.append(N_0w)\n",
    "            if \"lemma\" in token and self._check_informative(token[\"lemma\"]):\n",
    "                result.append(\"BUF_0_LEMMA_\" + token[\"lemma\"])\n",
    "            # N_0p\n",
    "            if self._check_informative(token[\"tag\"]):\n",
    "                N_0p = \"BUF_0_POS_\" + token[\"tag\"]\n",
    "                result.append(N_0p)\n",
    "            if \"feats\" in token and self._check_informative(token[\"feats\"]):\n",
    "                feats = token[\"feats\"].split(\"|\")\n",
    "                for feat in feats:\n",
    "                    result.append(\"BUF_0_FEATS_\" + feat)\n",
    "            # ADDED N_0wp\n",
    "            if self._check_informative(token[\"word\"], True) and self._check_informative(token[\"tag\"]):\n",
    "                N_0wp = \"BUF_0_FORM_POS_\" + token[\"word\"] + \"_\" + token[\"tag\"]\n",
    "                result.append(N_0wp)\n",
    "\n",
    "            # Buffer 1\n",
    "            if len(self.buffer) > 1:\n",
    "                buffer_idx1 = self.buffer[1]\n",
    "                token = self._tokens[buffer_idx1]\n",
    "                # N_1w\n",
    "                if self._check_informative(token[\"word\"], True):\n",
    "                    result.append(\"BUF_1_FORM_\" + token[\"word\"])\n",
    "                # N_1p\n",
    "                if self._check_informative(token[\"tag\"]):\n",
    "                    N_1p = \"BUF_1_POS_\" + token[\"tag\"]\n",
    "                    result.append(N_1p)\n",
    "\n",
    "                # ADDED N_1wp\n",
    "                if self._check_informative(token[\"word\"], True) and self._check_informative(token[\"tag\"]):\n",
    "                    result.append(\"BUF_1_FORM_POS_\" + token[\"word\"] + \"_\" + token[\"tag\"])\n",
    "\n",
    "            if len(self.buffer) > 2:\n",
    "                buffer_idx2 = self.buffer[2]\n",
    "                token = self._tokens[buffer_idx2]\n",
    "                # N_2p\n",
    "                if self._check_informative(token[\"tag\"]):\n",
    "                    N_2p = \"BUF_2_POS_\" + token[\"tag\"]\n",
    "                    result.append(N_2p)\n",
    "                # ADDED N_2w\n",
    "                if self._check_informative(token[\"word\"], True) and self._check_informative(token[\"tag\"]):\n",
    "                    result.append(\"BUF_2_FORM_\" + token[\"word\"])\n",
    "                # ADDED N_2wp\n",
    "                if self._check_informative(token[\"word\"], True) and self._check_informative(token[\"tag\"]):\n",
    "                    result.append(\"BUF_2_FORM_POS_\" + token[\"word\"] + \"_\" + token[\"tag\"])\n",
    "\n",
    "            if len(self.buffer) > 3:\n",
    "                buffer_idx3 = self.buffer[3]\n",
    "                token = self._tokens[buffer_idx3]\n",
    "                # N_3p\n",
    "                if self._check_informative(token[\"tag\"]):\n",
    "                    result.append(\"BUF_3_POS_\" + token[\"tag\"])\n",
    "                 # ADDED N_3w\n",
    "                if self._check_informative(token[\"word\"], True) and self._check_informative(token[\"tag\"]):\n",
    "                    result.append(\"BUF_3_FORM_\" + token[\"word\"])\n",
    "                # ADDED N_3wp\n",
    "                if self._check_informative(token[\"word\"], True) and self._check_informative(token[\"tag\"]):\n",
    "                    result.append(\"BUF_3_FORM_POS_\" + token[\"word\"] + \"_\" + token[\"tag\"])\n",
    "\n",
    "            # Left most, right most dependency of stack[0]\n",
    "            left_most = 1000000\n",
    "            right_most = -1\n",
    "            dep_left_most = \"\"\n",
    "            dep_right_most = \"\"\n",
    "            for (wi, r, wj) in self.arcs:\n",
    "                if wi == buffer_idx0:\n",
    "                    if (wj > wi) and (wj > right_most):\n",
    "                        right_most = wj\n",
    "                        dep_right_most = r\n",
    "                    if (wj < wi) and (wj < left_most):\n",
    "                        left_most = wj\n",
    "                        dep_left_most = r\n",
    "            if self._check_informative(dep_left_most):\n",
    "                result.append(\"BUF_0_LDEP_\" + dep_left_most)\n",
    "            if self._check_informative(dep_right_most):\n",
    "                result.append(\"BUF_0_RDEP_\" + dep_right_most)\n",
    "\n",
    "            # From word pairs\n",
    "            if S_0wp is not None and N_0wp is not None:\n",
    "                result.append(S_0wp + \"_\" + N_0wp)\n",
    "            if S_0wp is not None and N_0w is not None:\n",
    "                result.append(S_0wp + \"_\" + N_0w)\n",
    "            if S_0w is not None and N_0wp is not None:\n",
    "                result.append(S_0w + \"_\" + N_0wp)\n",
    "            if S_0wp is not None and N_0p is not None:\n",
    "                result.append(S_0wp + \"_\" + N_0p)\n",
    "            if S_0p is not None and N_0wp is not None:\n",
    "                result.append(S_0p + \"_\" + N_0wp)\n",
    "            if S_0w is not None and N_0w is not None:\n",
    "                result.append(S_0w + \"_\" + N_0w)\n",
    "            if S_0p is not None and N_0p is not None:\n",
    "                result.append(S_0p + \"_\" + N_0p)\n",
    "            if N_0p is not None and N_1p is not None:\n",
    "                result.append(N_0p + \"_\" + N_1p)\n",
    "\n",
    "            # From three words\n",
    "            if N_0p is not None and N_1p is not None and N_2p is not None:\n",
    "                result.append(N_0p + \"_\" + N_1p + \"_\" + N_2p)\n",
    "            if S_0p is not None and N_0p is not None and N_1p is not None:\n",
    "                result.append(S_0p + \"_\" + N_0p + \"_\" + N_1p)\n",
    "            if S_0h_p is not None and S_0p is not None and N_0p is not None:\n",
    "                result.append(S_0h_p + \" \" + S_0p + \"_\" + N_0p)\n",
    "            if S_0h_p is not None and S_0p is not None and N_0p is not None:\n",
    "                result.append(S_0h_p + \" \" + S_0p + \"_\" + N_0p)\n",
    "            if S_0p is not None and S_0l_p is not None and N_0p is not None:\n",
    "                result.append(S_0p + \" \" + S_0l_p + \"_\" + N_0p)\n",
    "            if S_0p is not None and S_0r_p is not None and N_0p is not None:\n",
    "                result.append(S_0p + \" \" + S_0r_p + \"_\" + N_0p)\n",
    "\n",
    "        return list(set(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ff5bb474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extend TransitionPaser to use my configuration class.\n",
    "class BetterTransitionParser(TransitionParser):\n",
    "    def _create_training_examples_arc_std(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : Page 32, Chapter 3. Dependency Parsing by Sandra Kubler, Ryan McDonal and Joakim Nivre (2009)\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_STANDARD)\n",
    "        count_proj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            count_proj += 1\n",
    "            conf = BetterConfiguration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + \":\" + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        precondition = True\n",
    "                        # Get the max-index of buffer\n",
    "                        maxID = conf._max_address\n",
    "\n",
    "                        for w in range(maxID + 1):\n",
    "                            if w != b0:\n",
    "                                relw = self._get_dep_relation(b0, w, depgraph)\n",
    "                                if relw is not None:\n",
    "                                    if (b0, relw, w) not in conf.arcs:\n",
    "                                        precondition = False\n",
    "\n",
    "                        if precondition:\n",
    "                            key = Transition.RIGHT_ARC + \":\" + rel\n",
    "                            self._write_to_file(key, binary_features, input_file)\n",
    "                            operation.right_arc(conf, rel)\n",
    "                            training_seq.append(key)\n",
    "                            continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(count_proj))\n",
    "        return training_seq\n",
    "\n",
    "    def _create_training_examples_arc_eager(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : 'A Dynamic Oracle for Arc-Eager Dependency Parsing' by Joav Goldberg and Joakim Nivre\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_EAGER)\n",
    "        countProj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            countProj += 1\n",
    "            conf = BetterConfiguration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + \":\" + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.RIGHT_ARC + \":\" + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.right_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # reduce operation\n",
    "                    flag = False\n",
    "                    for k in range(s0):\n",
    "                        if self._get_dep_relation(k, b0, depgraph) is not None:\n",
    "                            flag = True\n",
    "                        if self._get_dep_relation(b0, k, depgraph) is not None:\n",
    "                            flag = True\n",
    "                    if flag:\n",
    "                        key = Transition.REDUCE\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.reduce(conf)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(countProj))\n",
    "        return training_seq\n",
    "\n",
    "    def parse(self, depgraphs, modelFile):\n",
    "        \"\"\"\n",
    "        :param depgraphs: the list of test sentence, each sentence is represented as a dependency graph where the 'head' information is dummy\n",
    "        :type depgraphs: list(DependencyGraph)\n",
    "        :param modelfile: the model file\n",
    "        :type modelfile: str\n",
    "        :return: list (DependencyGraph) with the 'head' and 'rel' information\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        # First load the model\n",
    "        model = pickle.load(open(modelFile, \"rb\"))\n",
    "        operation = Transition(self._algorithm)\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            conf = BetterConfiguration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                features = conf.extract_features()\n",
    "                col = []\n",
    "                row = []\n",
    "                data = []\n",
    "                for feature in features:\n",
    "                    if feature in self._dictionary:\n",
    "                        col.append(self._dictionary[feature])\n",
    "                        row.append(0)\n",
    "                        data.append(1.0)\n",
    "                np_col = array(sorted(col))  # NB : index must be sorted\n",
    "                np_row = array(row)\n",
    "                np_data = array(data)\n",
    "\n",
    "                x_test = sparse.csr_matrix(\n",
    "                    (np_data, (np_row, np_col)), shape=(1, len(self._dictionary))\n",
    "                )\n",
    "\n",
    "                # It's best to use decision function as follow BUT it's not supported yet for sparse SVM\n",
    "                # Using decision function to build the votes array\n",
    "                # dec_func = model.decision_function(x_test)[0]\n",
    "                # votes = {}\n",
    "                # k = 0\n",
    "                # for i in range(len(model.classes_)):\n",
    "                #    for j in range(i+1, len(model.classes_)):\n",
    "                #        #if  dec_func[k] > 0:\n",
    "                #            votes.setdefault(i,0)\n",
    "                #            votes[i] +=1\n",
    "                #        else:\n",
    "                #           votes.setdefault(j,0)\n",
    "                #           votes[j] +=1\n",
    "                #        k +=1\n",
    "                # Sort votes according to the values\n",
    "                # sorted_votes = sorted(votes.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "                # We will use predict_proba instead of decision_function\n",
    "                prob_dict = {}\n",
    "                pred_prob = model.predict_proba(x_test)[0]\n",
    "                for i in range(len(pred_prob)):\n",
    "                    prob_dict[i] = pred_prob[i]\n",
    "                sorted_Prob = sorted(prob_dict.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "                # Note that SHIFT is always a valid operation\n",
    "                for (y_pred_idx, confidence) in sorted_Prob:\n",
    "                    # y_pred = model.predict(x_test)[0]\n",
    "                    # From the prediction match to the operation\n",
    "                    y_pred = model.classes_[y_pred_idx]\n",
    "\n",
    "                    if y_pred in self._match_transition:\n",
    "                        strTransition = self._match_transition[y_pred]\n",
    "                        baseTransition = strTransition.split(\":\")[0]\n",
    "\n",
    "                        if baseTransition == Transition.LEFT_ARC:\n",
    "                            if (\n",
    "                                operation.left_arc(conf, strTransition.split(\":\")[1])\n",
    "                                != -1\n",
    "                            ):\n",
    "                                break\n",
    "                        elif baseTransition == Transition.RIGHT_ARC:\n",
    "                            if (\n",
    "                                operation.right_arc(conf, strTransition.split(\":\")[1])\n",
    "                                != -1\n",
    "                            ):\n",
    "                                break\n",
    "                        elif baseTransition == Transition.REDUCE:\n",
    "                            if operation.reduce(conf) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.SHIFT:\n",
    "                            if operation.shift(conf) != -1:\n",
    "                                break\n",
    "                    else:\n",
    "                        raise ValueError(\n",
    "                            \"The predicted transition is not recognized, expected errors\"\n",
    "                        )\n",
    "\n",
    "            # Finish with operations build the dependency graph from Conf.arcs\n",
    "\n",
    "            new_depgraph = deepcopy(depgraph)\n",
    "            for key in new_depgraph.nodes:\n",
    "                node = new_depgraph.nodes[key]\n",
    "                node[\"rel\"] = \"\"\n",
    "                # With the default, all the token depend on the Root\n",
    "                node[\"head\"] = 0\n",
    "            for (head, rel, child) in conf.arcs:\n",
    "                c_node = new_depgraph.nodes[child]\n",
    "                c_node[\"head\"] = head\n",
    "                c_node[\"rel\"] = rel\n",
    "            result.append(new_depgraph)\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ebf011",
   "metadata": {},
   "source": [
    "#### Q 4.2\n",
    "\n",
    "-   Evaluate the features comparing performance to the original.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6e5177f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package dependency_treebank to\n",
      "[nltk_data]     /Users/filippomomesso/nltk_data...\n",
      "[nltk_data]   Package dependency_treebank is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original nltk TransitionParser\n",
      " Number of training examples : 100\n",
      " Number of valid (projective) examples : 100\n",
      "Unlabeled Attachment Score: 0.7708333333333334\n",
      "Better features TransitionParser\n",
      " Number of training examples : 100\n",
      " Number of valid (projective) examples : 100\n",
      "Unlabeled Attachment Score with better features: 0.7916666666666666\n",
      "Gain in UAS: 0.02083333333333326\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('dependency_treebank')\n",
    "from nltk.parse.transitionparser import TransitionParser\n",
    "from nltk.parse import DependencyEvaluator\n",
    "from nltk.corpus import dependency_treebank\n",
    "\n",
    "# make train and test sets from dependency treebank\n",
    "train_parsed_sents = dependency_treebank.parsed_sents()[:100]\n",
    "test_parsed_sents = dependency_treebank.parsed_sents()[-10:]\n",
    "\n",
    "#Train and evaluate original nltk Transition Parser\n",
    "print(\"Original nltk TransitionParser\")\n",
    "tp_original = TransitionParser('arc-eager')\n",
    "tp_original.train(train_parsed_sents, 'tp.model-original', verbose=False)\n",
    "parses_original = tp_original.parse(test_parsed_sents, 'tp.model-original')\n",
    "_, uas_original = DependencyEvaluator(parses_original, test_parsed_sents).eval()\n",
    "print(f\"Unlabeled Attachment Score: {uas_original}\")\n",
    "\n",
    "# Train and evaluate original my parser\n",
    "print(\"Better features TransitionParser\")\n",
    "tp_better_feats = BetterTransitionParser('arc-eager')\n",
    "tp_better_feats.train(train_parsed_sents, 'tp.model-better', verbose=False)\n",
    "parses_better_feats = tp_better_feats.parse(test_parsed_sents, 'tp.model-better')\n",
    "_, uas_better_feats = DependencyEvaluator(parses_better_feats, test_parsed_sents).eval()\n",
    "print(f\"Unlabeled Attachment Score with better features: {uas_better_feats}\")\n",
    "\n",
    "print(f\"Gain in UAS: {uas_better_feats-uas_original}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa4657c",
   "metadata": {},
   "source": [
    "#### Q 4.3\n",
    "\n",
    "-   Replace SVM classifier with an alternative of your choice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "93b94966",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "import pickle\n",
    "import tempfile\n",
    "from copy import deepcopy\n",
    "from operator import itemgetter\n",
    "from os import remove\n",
    "\n",
    "# Extend TransitionParser to override train method in order to use Adaboost classifier\n",
    "class AdaboostTransitionParser(TransitionParser):\n",
    "    def train(self, depgraphs, modelfile, verbose=True):\n",
    "        \"\"\"\n",
    "        :param depgraphs : list of DependencyGraph as the training data\n",
    "        :type depgraphs : DependencyGraph\n",
    "        :param modelfile : file name to save the trained model\n",
    "        :type modelfile : str\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            input_file = tempfile.NamedTemporaryFile(\n",
    "                prefix=\"transition_parse.train\", dir=tempfile.gettempdir(), delete=False\n",
    "            )\n",
    "\n",
    "            if self._algorithm == self.ARC_STANDARD:\n",
    "                self._create_training_examples_arc_std(depgraphs, input_file)\n",
    "            else:\n",
    "                self._create_training_examples_arc_eager(depgraphs, input_file)\n",
    "\n",
    "            input_file.close()\n",
    "            # Using the temporary file to train the libsvm classifier\n",
    "            x_train, y_train = load_svmlight_file(input_file.name)\n",
    "\n",
    "            # An AdaBoost classifier is a meta-estimator that begins\n",
    "            # by fitting a classifier on the original dataset\n",
    "            # and then fits additional copies of the classifier on the same dataset\n",
    "            # but where the weights of incorrectly classified instances are adjusted\n",
    "            # such that subsequent classifiers focus more on difficult cases.\n",
    "            model = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "            model.fit(x_train, y_train)\n",
    "\n",
    "            # Save the model to file name (as pickle)\n",
    "            pickle.dump(model, open(modelfile, \"wb\"))\n",
    "        finally:\n",
    "            remove(input_file.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fcd57416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost classifier TransitionParser\n",
      " Number of training examples : 100\n",
      " Number of valid (projective) examples : 100\n",
      "Unlabeled Attachment Score: 0.6\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate Transition Parser with Adaboost classifier\n",
    "print(\"Adaboost classifier TransitionParser\")\n",
    "tp_nb = AdaboostTransitionParser('arc-eager')\n",
    "tp_nb.train(train_parsed_sents, 'tp.model', verbose=False)\n",
    "parses_nb = tp_nb.parse(test_parsed_sents, 'tp.model')\n",
    "_, uas_nb = DependencyEvaluator(parses_nb, test_parsed_sents).eval()\n",
    "print(f\"Unlabeled Attachment Score: {uas_nb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd289fdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5d9a352f5f9c67829ebf8b3c2bc7cdb290dff4a3b1eff8220e7b57a4502d3813"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
